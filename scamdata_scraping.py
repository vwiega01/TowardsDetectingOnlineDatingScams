# -*- coding: utf-8 -*-
"""ScamData_Scraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bq_o5qooLRmlmbvCV759mkxkRG2ITTyj

# Exploring the Dataset
[Github Repo: Dataset](https://github.com/gsuareztangil/automatic-romancescam-digger/blob/master/data/scripts/realdownloader.py)
"""

# drive already mounted
import google.colab
from google.colab import drive
drive.mount('/content/drive/')

"""## Scraping Scam Profiles"""

# imports for scamdownloader.py
import os
import re
import json
import time
import hashlib
import random
from bs4 import BeautifulSoup
from urllib.request import urlopen
import urllib

# code from scam downloader.py
IMAGEDIR='/content/drive/MyDrive/VURF/Google_Collabs/JSON_files/scam_images'
PROFILES='/content/drive/MyDrive/VURF/Google_Collabs/JSON_files/scam'


extractors = {'username': re.compile('username: ([^\n]+)'),
              'name': re.compile('\Wname: ([^\n]+)'),
              'age': re.compile('\Wage: ([^\n]+)'),
              'location': re.compile('\Wlocation: ([^\n]+)'),
              'ethnicity': re.compile('\Wethni?city: ([^\n]+)'),
              'occupation': re.compile('\Woccupation: ([^\n]+)'),
              'status': re.compile('\Wmarital status: ([^\n]+)'),
              'phone': re.compile('\Wtel: ([^\n]+)'),
              'inet': re.compile('\WIP address: ([^\n]+)'),
              'email': re.compile('\Wemail: ([^\n]+)'),
              'description': re.compile('\Wdescription:([\n\w\W]+)\Wmessage:'),
              'messages': re.compile('\Wmessage:([\n\w\W]+)\WWHY IS'),
              'justifications': re.compile('\WWHY IS IT A SCAM / FAKE:([\n\w\W]+)\W This post')}

def striphtml(data):
    p = re.compile(r'<.*?>')
    return p.sub('', data)

def save_image(url):
    """ Take a URL, generate a unique filename, save 
        the image to said file and return the filename."""
    ext = url.split('.')[-1]
    filename = IMAGEDIR+os.sep+hashlib.md5(url.encode('utf-8')).hexdigest()+'.'+ext
    if os.path.exists(filename):
        return filename
    try:
        content = urlopen(url).read()
        f = open(filename,'wb') 
        f.write(content)
        f.close()
    except:
        return None
    return filename 


def scrape_profile(inhandle, outfile, year, month):
  """Scrape an input scamdiggers page for the profile content
  of the scammer. """


  #Read file
  html = inhandle.read()
  soup = BeautifulSoup(html, 'html.parser')

  #Find main page content
  content = soup.find('div', {'class':'entry-content'})

  text = [link for link in soup.find_all('p')]

  # print(text[1])


  


  profile = {}

  #Fill in known info from URL
  profile['year_reported'] = year
  profile['month_reported'] = month

  #Extract and download images.
  profile['images'] = [save_image(img['src']) for img in soup.findAll('img')]

  # these 4 lines were commented off
  # #Get visible text
  # v = ' '.join(text)
  # text = content.get_text().strip()

  # print(text)

  #Parse information from text

  # print(extractors)
  for key in extractors:
    for value in text:
      match = extractors[key].search(str(value))
      # print(match)
      if match:
        matchtext = match.group(1).strip()
        if key in ['justifications','messages']:
          vals = matchtext.split('\n')
        else:
          vals = matchtext
        profile[key] = striphtml(vals)

  # print(profile) 

  #Parse annotations
  content = soup.find('div', {'class':'entry-utility'})
  profile['tags']   = [node.get_text() for node in content.findAll('a', {'rel':'tag'})]
  profile['gender'] = 'female' if 'Female profiles' in profile['tags'] else 'male'

  #Save output
  json.dump(profile, open(outfile,'w'))


def enumerate_profiles(inhandle, page):
  """ Extract all the profile page links from
  this index page. """
  html = inhandle.read()
  soup = BeautifulSoup(html, 'html.parser')
  # print(soup.findAll('h1',  {'class':'entry-title'}))

  urls = []
  
  # if soup.findAll('h1',  {'class':'entry-title'}) is not None:
  # urls = [ node.find('a')['href'] for node in soup.findAll('h1',  {'class':'entry-title'}) if node is not None]
  

  for name in soup.findAll('h1',  {'class':'entry-title'}):
    # urls = name.find_all("a", href=True)
    urls += [link['href'] for link in name.find_all('a')]
    print (urls) # was commented out

  # for name in soup.find_all("td",class_="result_text"):
  #   title = name.find_all("a",text=True)[0]
  #   print (title.text)
  # # print(html)

  # print(urls)
  return urls


def gather_all_profiles(year, month):
  """ Walk the index pages, harvesting the profile URLs,
  and then download and process all the profiles stored 
  under this year and month. """

  hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',
       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',
       'Accept-Encoding': 'none',
       'Accept-Language': 'en-US,en;q=0.8',
       'Connection': 'keep-alive'}

  urllib.request.urlcleanup()

  page = 1
  # print(page)
  urls = []

  print("{}-{} : Begin indexing.".format(year, month))

  while (page > 0):
    urlstring = "http://scamdigger.com/{}/{}/page/{}".format(year,month,page)  # ORIGINAL: "http://scamdigger.com/{}/{}/page/{}".format(year,month,page) 
    print(urlstring)

    jitter = random.choice([0,1])
    try:
      req = urllib.request.Request(urlstring, headers=hdr)
      urlhandle = urllib.request.urlopen(req)    
      print('page:'+ str(page))
      urls += enumerate_profiles(urlhandle, page)
      time.sleep(1+jitter)
      page += 1
    except urllib.error.HTTPError as e:
      page = 0
      # return e
    
  print("{}-{} : {} profiles".format(year,month,len(urls)))

  for url in urls:
    uid = url[30:-1]
    outfile=PROFILES+os.sep+uid+'.json'
    print(str(outfile))
    jitter = random.choice([0,1])
    try:
      urlhandle = urlopen(url)
      scrape_profile(urlhandle, outfile, year, month)
      time.sleep(1+jitter)
    except Exception as e:
      print("Exception when handling {}".format(url))
      print(e)
  
  print("{}-{} : complete.".format(year,month))


def scrape(startyear, startmonth, endyear, endmonth):
  """ Walk the database through the defined ranges,
  downloading everything. """
  year = startyear
  month = startmonth
  while (not (year == endyear and month == endmonth)):
    ys = "{}".format(year)
    ms = "{:02d}".format(month)
    gather_all_profiles(ys,ms) 
    if month == 12:
      year += 1
      month = 0
    month += 1


# scrape(2015,7,2022,7)
scrape(2016,7,2022,7)